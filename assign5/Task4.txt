Ques 1. What is entropy and information gain?

Ans - Entropy is a measure of randomness or impurity in a dataset. It tells us how mixed the class labels are.
Information Gain is the reduction in entropy after splitting the data on a feature â€” it helps us decide which feature to split on in a decision tree.

Ques 2. Explain the difference between Gini Index and Entropy.

Ans - 
    Both Gini and Entropy measure impurity, but in slightly different ways.
    Entropy uses logarithms and can be a bit more computationally expensive.
    Gini Index is simpler and faster, and usually gives similar results.
    In practice, they often lead to similar trees.

Ques 3. How can a decision tree overfit? How can this be avoided?

Ans - 
    A decision tree can overfit when it grows too deep and starts memorizing the training data, including noise.
    To avoid this, we can:
    Limit the tree depth
    Use pruning
    Set a minimum number of samples per leaf
    Use ensemble methods like Random Forest