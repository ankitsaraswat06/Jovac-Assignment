Ques 1. What is the difference between Bagging and Boosting?

Ans - Bagging trains multiple models independently in parallel on different random subsets of the data (with replacement), and then combines their outputs, usually by voting or averaging.
Boosting, on the other hand, trains models sequentially, where each new model tries to fix the errors made by the previous ones. So, bagging reduces variance, while boosting focuses more on reducing bias.

Ques 2. How does Random Forest reduce variance?

Ans - Random Forest is basically a collection of decision trees trained on different random subsets of the data and features. By averaging the predictions of many uncorrelated trees, it cancels out the noise and overfitting that a single decision tree might have — which helps reduce variance and improves generalization.

Ques 3. What is the weakness of boosting-based methods?

Ans - Boosting can be sensitive to noisy data and outliers, since it tries hard to correct previous mistakes — which sometimes includes fitting to noise. Also, it’s generally more computationally expensive and harder to parallelize compared to bagging methods like Random Forest.
